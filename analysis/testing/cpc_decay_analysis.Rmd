```{r}
# Imports
library(tidyverse)
library(pracma)
library(dplyr)
library(openair)
library(forecastML)
library(zoom)
```

```{r, setup, include=FALSE}
# set working directory
knitr::opts_knit$set(root.dir = '/home/sjatti/Desktop/hepa-summer23')
```

```{r}
# check file path for working directory
if (!endsWith(getwd(), "hepa-summer23")) {
  stop("Incorrect working directory")
}
```

```{r}
# Set path to data
path_to_data <- "data/HAFTRAP/OH/cpc/"

# Load modpm data from RData file
load(paste0(path_to_data, "cleaned_cpc.RData"))
```

```{r}
# vector of participant numbers' data to be processed (modify to analyze specific participants)
participants <- "45451"
cases = "sham"
# ("41271", "42281", "44621", 45361", "45451")
```

```{r}
# Filter data to appropriate subset
df <- main_df %>%
  filter(participant_id == participants, case == cases, 
         environment == "indoor")
```

```{r}
df$date <- round_date(df$date, unit = "10 mins")
```

```{r}
df <- df %>%
  fill_gaps(frequency = "10 min") %>%
  mutate(t = as.numeric(date) - as.numeric(date)[1]) %>%
  fill(concent)
```

```{r}
# function to create a new dataframe with summary data
summary_dataframe <- function(data) {
  data %>%
    
  # group by 10-min intervals
  group_by(date) %>%
    
  # calculate mean for all groups
  summarise(mean = mean(concent))
}
```

```{r}
# create a summary dataframe
sum_df <- summary_dataframe(df)
```

```{r}
# find peaks for each particle and store in new dataframes
peaks <- findpeaks(sum_df$mean,
                   nups = 1,
                   ndowns = 1,
                   minpeakheight = 5000,
                   minpeakdistance = 20,
                   threshold = 0)
```

```{r}
# Function to get first valley after peak
get_first_valleys <- function(data, peaks, min_threshold) {
  if (is.null(peaks)) {
    return(print("There are no peaks/valleys"))
  }
  # Iterate through rows of peaks matrix
  valley_mat <- matrix(nrow = nrow(peaks), ncol = 2)
  for (row in 1:nrow(peaks)) { #nolint
    idx <- peaks[row, 2]
    n <- TRUE
    # create while loop to check for local minima
    while (n) {
      # if we're at the end of y, break out of loop
      if (idx == length(data)) {
        n <- FALSE
      } else {
        # otherwise, get slope (approximately)
        # checks that next three points are going up to consider it a valley
        slope <- data[idx + 1] - data[idx]
        slope2 <- data[idx + 2] - data[idx + 1]
        slope3 <- data[idx + 3] - data[idx + 2]
        if (data[idx] <= min_threshold && slope > 0) {
          n <- FALSE
        } else {
          idx <- idx + 1
        }
      }
    }
    # set values of matrix so that column 1 is height of valley and 2 is index
    valley_mat[row, 1] <- data[idx]
    valley_mat[row, 2] <- idx
  }
  valley_mat
}
```

```{r}
# Find the first valley after each peak
valley_threshold = 15000

valleys <- get_first_valleys(sum_df$mean, peaks, valley_threshold)
```

```{r}
# function for peak/valley filtering - if multiple peaks detect the same valley, remove everything but the last one
filter_peaks <- function(peaks, valleys) {
  
  peaks_valleys <- matrix(nrow = nrow(peaks), ncol = 4)
  
  peaks <- as.data.frame(peaks)
  valleys <- as.data.frame(valleys)
  
  valleys <- valleys %>%
    arrange(desc(V2))
  peaks <- peaks %>%
    arrange(desc(V2))
  
  if (nrow(peaks) == 1){
    peaks_valleys <- cbind(select(peaks, V1, V2), valleys)
  } else {
  
    for (row in 1:(nrow(peaks) - 1)) {
      if (valleys[[row, 1]] != valleys[[row + 1, 1]]) {
        
        peaks_valleys[row, 1] <- peaks[row, 1]
        peaks_valleys[row, 2] <- peaks[row, 2]
        
        peaks_valleys[row, 3] <- valleys[row, 1]
        peaks_valleys[row, 4] <- valleys[row, 2]
      }
    }
    peaks_valleys[nrow(peaks), 1] <- peaks[nrow(peaks), 1]
    peaks_valleys[nrow(peaks), 2] <- peaks[nrow(peaks), 2]
        
    peaks_valleys[nrow(peaks), 3] <- valleys[nrow(peaks), 1]
    peaks_valleys[nrow(peaks), 4] <- valleys[nrow(peaks), 2]
  }
  
  peaks_valleys <- na.omit(peaks_valleys)
  colnames(peaks_valleys) <- c("peaks_y", "peaks_x", "valleys_y", "valleys_x")

  peaks_valleys <- as.data.frame(peaks_valleys)

  # remove row if valley > peak
  peaks_valleys$results = ifelse(peaks_valleys$valleys_y > peaks_valleys$peaks_y | peaks_valleys$valleys_y > valley_threshold, NA, 1)
  peaks_valleys <- drop_na(peaks_valleys)
                
  return(peaks_valleys)
}
```

```{r}
peaks_valleys <- filter_peaks(peaks, valleys)
```

```{r}
# function for exponential curve fitting for air quality data; returns 
# dataframe containing k-values of curves
curve_fitting <- function(data, peaks_valleys) {
    # Create empty dataframe for storing k values
  alphas.data <- data.frame(
    "peak_idx" = numeric(0),
    "valley_idx" = numeric(0),
    "peak_hgt" = numeric(0),
    "k_val" = numeric(0),
    "conv_tol" = numeric(0)
  )
  # Define parameters for curve fitting function for each row
 for (row in 1:nrow(peaks_valleys)) 
  {
    i_range <- peaks_valleys[row, 2]:peaks_valleys[row, 4]
    sect <- data[i_range]
    t <- i_range - peaks_valleys[row, 2] + 1
    main_df <- data.frame(t = t, y = sect)
    
    alphas.newdata <- tryCatch({
      
      # get exponential fit
      fit <- nls(y ~ SSasymp(t, yf, y0, log_alpha), data = main_df)
      
      # Get parameters of the fit
      params <- coef(fit)
      
      # Extract the log_alpha value and put it in form e^(log(a)) to get a
      log_alpha <- as.double(params["log_alpha"])
      alpha <- exp(log_alpha)
      
      # Get achieved convergence tolerance as metric for accuracy of fit
      # NOTE: R^2 value can be calculated but is not a useful metric
      # for nonlinear models
      conv <- fit$convInfo$finTol
      # Add alpha to dataframe
      alphas.newdata <- data.frame(
        "peak_idx" = c(peaks_valleys[row, 2]),
        "valley_idx" = c(peaks_valleys[row, 4]),
        "peak_hgt" = c(peaks_valleys[row, 1]),
        "k_val" = c(alpha),
        "conv_tol" = c(conv)
      )
      }, error = function(e) { # print(e)
      }, warning = function(w) {
      }, finally = {
      })
      print(class(alphas.newdata))
      alphas.data <- rbind(alphas.data, alphas.newdata)
  }
  alphas.data <- arrange_all(alphas.data)
  return(alphas.data)
}
```

```{r}
# Function to plot the peaks and valleys on top of current time series
plot_peaks_valleys <- function(data, peaks_valleys) {
  plot(data,
  type = "l",
  main = "PM Concentration Over Time",
  xlab = "Time (min)",
  ylab = "Concentration",
  col = "navy")
  grid()
points(peaks_valleys[, 2], peaks_valleys[, 1], pch = 20, col = "maroon")
points(peaks_valleys[, 4], peaks_valleys[, 3], pch = 20, col = "green")
}
```

```{r}
# Plot the time series with the peaks and first valleys plotted on top
plot_peaks_valleys(sum_df$mean, peaks_valleys)
```

```{r}
# Find the decay value (k constant) for each peak
decays <- curve_fitting(sum_df$mean, peaks_valleys)
```



<!-- ```{r} -->
<!-- modify_df <- function(df) { -->
<!--   df %>% -->
<!--     add_column(case = cases) %>% -->
<!--     add_column(participant = participants) -->
<!-- } -->
<!-- ``` -->

<!-- ```{r} -->
<!-- decay_constants <- modify_df(decays) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- # add to overall df and save -->
<!-- decay_constants_final <- rbind(decay_constants_final, decay_constants) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- save(decay_constants_final, file = paste0(path_to_data, "decay_constants_cpc.RData")) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- # decay_constants_final <- matrix(nrow = 0, ncol = 4) -->
<!-- ``` -->

